{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save JSON Cache File\n",
    "\n",
    "This notebook processes the JSON files into index tokens and document vectors and save to cache files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from cord import ResearchPapers\n",
    "from cord.core import find_data_dir, JSON_CATALOGS, cord_cache_dir, parallel\n",
    "from cord.jsonpaper import load_tokens_from_file, get_json_paths\n",
    "from pathlib import Path, PurePath\n",
    "from cord.text import preprocess\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.options.display.max_columns = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load ResearchPapers and get metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading metadata from ..\\data\\CORD-19-research-challenge\n",
      "Cleaning metadata\n",
      "Applying tags to metadata\n",
      "\n",
      "Indexing research papers\n",
      "Creating the BM25 index from the abstracts of the papers\n",
      "Use index=\"text\" if you want to index the texts of the paper instead\n",
      "Finished Indexing in 70.0 seconds\n"
     ]
    }
   ],
   "source": [
    "papers = ResearchPapers.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create Index Tokens and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cord.jsonpaper import get_token_df\n",
    "from cord.core import split_df\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "def save_dictionary(dictionary, save_path):\n",
    "    print('Saving dictionary to', save_path)\n",
    "    with save_path.open('wb') as f:\n",
    "        dictionary.save(f)\n",
    "        \n",
    "def token_2ints(json_text_df):\n",
    "    dictionary = Dictionary(json_text_df.index_tokens)\n",
    "    json_text_df['token_int'] \\\n",
    "            = json_text_df.index_tokens.apply(lambda tokens:  [dictionary.token2id[t] for t in tokens])\n",
    "    return dictionary, json_text_df.drop(columns=['index_tokens'])\n",
    "\n",
    "def save_json_cache_files():\n",
    "    json_cache_path = Path(find_data_dir()).parent / 'json-cache'\n",
    "    if json_cache_path.exists():\n",
    "        print('Json Cache dir exists')\n",
    "        for cache_file in json_cache_path.glob('*.*'):\n",
    "            print('Removing', cache_file)\n",
    "            cache_file.unlink()\n",
    "        for dict_file in json_cache_path.glob('*.dict'):\n",
    "            print('Removing', dict_file)\n",
    "            dict_file.unlink()\n",
    "    else:\n",
    "        print('Creating directory', json_cache_path)\n",
    "        json_cache_path.mkdir(exist_ok=True)\n",
    "\n",
    "    metadata_parts = split_df(papers.metadata, 15000)\n",
    "    for i, df in enumerate(metadata_parts):\n",
    "        json_text_df = get_token_df(df, papers.data_path)\n",
    "        dictionary, json_text_df = token_2ints(json_text_df)\n",
    "        save_dictionary(dictionary, json_cache_path / f'jsoncache_{i}.dict' )\n",
    "        catalog_save_path = json_cache_path / f'jsoncache_{i}.pq'\n",
    "        print('Saving to', catalog_save_path)\n",
    "        json_text_df.to_parquet(catalog_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Json Cache dir exists\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6c6e3b5b5da4026ad999dd6cf2f2306",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=15000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving dictionary to ..\\data\\json-cache\\jsoncache_0.dict\n",
      "Saving to ..\\data\\json-cache\\jsoncache_0.pq\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9086b50df97c458793d474941f542afa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=15000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving dictionary to ..\\data\\json-cache\\jsoncache_1.dict\n",
      "Saving to ..\\data\\json-cache\\jsoncache_1.pq\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e98902f453b54e1c8e2491f355a02608",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=15000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving dictionary to ..\\data\\json-cache\\jsoncache_2.dict\n",
      "Saving to ..\\data\\json-cache\\jsoncache_2.pq\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a1edf9fec604ab5987902f42caa3a53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=15000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving dictionary to ..\\data\\json-cache\\jsoncache_3.dict\n",
      "Saving to ..\\data\\json-cache\\jsoncache_3.pq\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23f29795160b45829ec893c36e5d5a84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3571.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving dictionary to ..\\data\\json-cache\\jsoncache_4.dict\n",
      "Saving to ..\\data\\json-cache\\jsoncache_4.pq\n"
     ]
    }
   ],
   "source": [
    "save_json_cache_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading json cache file jsoncache_0\n",
      "Loading json cache file jsoncache_1\n",
      "Loading json cache file jsoncache_2\n",
      "Loading json cache file jsoncache_3\n",
      "Loading json cache file jsoncache_4\n",
      "Loaded json cache in 83 seconds\n"
     ]
    }
   ],
   "source": [
    "from cord.jsonpaper import get_json_cache_dir\n",
    "import time\n",
    "def index_from_jsoncache(metadata):\n",
    "    json_cache_dir = get_json_cache_dir()\n",
    "    file_paths = [PurePath(p) for p in json_cache_dir.glob(f'jsoncache_*.pq')]\n",
    "    tick = time.time()\n",
    "    for cache_path in file_paths:\n",
    "        print('Loading json cache file', cache_path.stem)\n",
    "        json_cache = pd.read_parquet(cache_path)\n",
    "        part_no = cache_path.stem[len('jsoncache_'):]\n",
    "        dictionary_path = json_cache_dir / f'jsoncache_{part_no}.dict'\n",
    "        dictionary = Dictionary.load((str(dictionary_path.resolve())))\n",
    "        json_cache['index_tokens'] \\\n",
    "                    = json_cache.token_int.apply(lambda token_int: [dictionary[ti] for ti in token_int])\n",
    "        json_tokens = json_cache.drop(columns=['token_int'])\n",
    "        token_lookup = json_tokens.to_dict()['index_tokens']\n",
    "        need_tokens = metadata.index_tokens.isnull()\n",
    "        metadata.loc[need_tokens, 'index_tokens'] = \\\n",
    "            metadata.loc[need_tokens, 'cord_uid'].apply(lambda c: token_lookup.get(c, np.nan))\n",
    "        \n",
    "    tock = time.time()\n",
    "    print('Loaded json cache in', int(tock - tick), 'seconds')\n",
    "        \n",
    "index_from_jsoncache(papers.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Research Papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading metadata from ..\\data\\CORD-19-research-challenge\n",
      "Cleaning metadata\n",
      "Applying tags to metadata\n",
      "\n",
      "Indexing research papers\n",
      "Creating the BM25 index from the text contents of the papers\n",
      "Loading json cache file jsoncache_0\n",
      "Loading json cache file jsoncache_1\n",
      "Looking for 48777 tokens\n",
      "Loading json cache file jsoncache_2\n",
      "Looking for 39333 tokens\n",
      "Loading json cache file jsoncache_3\n",
      "Looking for 30190 tokens\n",
      "Loading json cache file jsoncache_4\n",
      "Looking for 16918 tokens\n",
      "There are 13688 papers that will be indexed using the abstract instead of the contents\n",
      "Finished indexing in 92 seconds\n"
     ]
    }
   ],
   "source": [
    "papers = ResearchPapers.load(index='text')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cord",
   "language": "python",
   "name": "cord"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
